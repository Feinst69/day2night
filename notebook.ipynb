{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436c43e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/python/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/python/lib/python3.12/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/python/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/python/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/python/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/python/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ca6f73",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mboto3\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mboto3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01ms3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransfer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TransferConfig\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from dotenv import load_dotenv\n",
    "from botocore.exceptions import ClientError\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Union, BinaryIO\n",
    "import mimetypes\n",
    "\n",
    "import json\n",
    "import io\n",
    "import math\n",
    "import sys\n",
    "import threading\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c055daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'Metadata_Export_20251124_184630.csv',\n",
    "    sep=';',\n",
    "    quotechar='\"',\n",
    "    engine='python'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ext'] = df['Nom'].str.split('.').str[-1].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f812e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ext\n",
       "pdf     3445\n",
       "docx     568\n",
       "xlsm     162\n",
       "pptx      44\n",
       "xlsx      31\n",
       "doc        3\n",
       "log        1\n",
       "docm       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ext\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a8f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3.fr-par.scw.cloud\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path='../backend/.env')  # loads .env into environment variables\n",
    "\n",
    "print(os.getenv(\"S3_ENDPOINT_URL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8cc98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8583e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 1575 (2478932145.py, line 1679)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1679\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef upload_raw_data_to_s3(local_raw_data_dir, project_bucket, mailbox_name):\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'if' statement on line 1575\n"
     ]
    }
   ],
   "source": [
    "# %load \n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'Metadata_Export_20251124_184630.csv',\n",
    "    sep=';',\n",
    "    quotechar='\"',\n",
    "    engine='python'\n",
    ")\n",
    "df['ext'] = df['Nom'].str.split('.').str[-1].str.lower()\n",
    "df[\"ext\"].value_counts()\n",
    "import os\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from dotenv import load_dotenv\n",
    "from botocore.exceptions import ClientError\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Union, BinaryIO\n",
    "import mimetypes\n",
    "\n",
    "import json\n",
    "import io\n",
    "import math\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class UploadProgress:\n",
    "    \"\"\"Console progress reporter for S3 uploads.\"\"\"\n",
    "\n",
    "    def __init__(self, filename: str):\n",
    "        self.filename = filename\n",
    "        self.filesize = float(os.path.getsize(filename)) if os.path.exists(filename) else 0.0\n",
    "        self._seen_so_far = 0\n",
    "        self._lock = threading.Lock()\n",
    "        self._start_time = time.time()\n",
    "\n",
    "    def __call__(self, bytes_amount: int) -> None:\n",
    "        if self.filesize <= 0:\n",
    "            return\n",
    "        with self._lock:\n",
    "            self._seen_so_far += bytes_amount\n",
    "            elapsed = max(time.time() - self._start_time, 1e-6)\n",
    "            speed = self._seen_so_far / elapsed  # bytes/sec\n",
    "            remaining = max(self.filesize - self._seen_so_far, 0.0)\n",
    "            eta = remaining / speed if speed > 0 else float(\"inf\")\n",
    "            percentage = (self._seen_so_far / self.filesize) * 100.0\n",
    "\n",
    "            eta_str = f\"{eta:6.1f}s\" if math.isfinite(eta) else \"--.-s\"\n",
    "\n",
    "            line = (\n",
    "                f\"\\r[{os.path.basename(self.filename)}] \"\n",
    "                f\"{self._seen_so_far/1e6:8.1f} / {self.filesize/1e6:8.1f} MB \"\n",
    "                f\"({percentage:5.1f}%)  speed: {speed/1e6:5.1f} MB/s  ETA: {eta_str}\"\n",
    "            )\n",
    "            sys.stdout.write(line)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if self._seen_so_far >= self.filesize:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "\n",
    "class S3Handler:\n",
    "    \"\"\"\n",
    "    A class to handle common S3 operations using boto3.\n",
    "    This includes listing, creating, deleting buckets and objects,\n",
    "    uploading and downloading files, and generating presigned URLs.\n",
    "    It also supports using environment variables for configuration.\n",
    "    Environment variables:\n",
    "        - S3_ENDPOINT_URL: The endpoint URL for the S3 service\n",
    "        - S3_REGION_NAME: The region name for the S3 service\n",
    "        - SCW_ACCESS_KEY: Access key ID for authentication\n",
    "        - SCW_SECRET_KEY: Secret access key for authentication\n",
    "    Example usage:\n",
    "        s3_handler = S3Handler()\n",
    "        buckets = s3_handler.list_buckets()\n",
    "        print(buckets)\n",
    "        s3_handler.create_bucket('my-new-bucket')\n",
    "        s3_handler.upload_file('local_file.txt', 'my-new-bucket', 's3_file.txt')\n",
    "        url = s3_handler.generate_presigned_url('my-new-bucket', 's3_file.txt')\n",
    "        print(url)\n",
    "        s3_handler.download_file('my-new-bucket', 's3_file.txt', 'downloaded_file.txt')\n",
    "    This class requires the `boto3` and `python-dotenv` packages.\n",
    "    Install them using:\n",
    "        pip install boto3 python-dotenv\n",
    "    Ensure to set the environment variables or pass them as arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, endpoint_url=None, region_name=None,\n",
    "                 access_key_id=None, secret_access_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the S3 handler with optional credentials.\n",
    "        If not provided, will use environment variables.\n",
    "        \"\"\"\n",
    "        # Load environment variables if not done already\n",
    "        load_dotenv()\n",
    "\n",
    "        # Use provided credentials or fall back to environment variables\n",
    "        self.endpoint_url = endpoint_url or os.getenv(\"S3_ENDPOINT_URL\")\n",
    "        self.region_name = region_name or os.getenv(\"S3_REGION_NAME\")\n",
    "        self.access_key_id = access_key_id or os.getenv(\"SCW_ACCESS_KEY\")\n",
    "        self.secret_access_key = secret_access_key or os.getenv(\"SCW_SECRET_KEY\")\n",
    "\n",
    "        # Initialize S3 resource and client\n",
    "        self.s3 = boto3.resource(\n",
    "            service_name='s3',\n",
    "            endpoint_url=self.endpoint_url,\n",
    "            region_name=self.region_name,\n",
    "            aws_access_key_id=self.access_key_id,\n",
    "            aws_secret_access_key=self.secret_access_key\n",
    "        )\n",
    "\n",
    "        self.client = boto3.client(\n",
    "            service_name='s3',\n",
    "            endpoint_url=self.endpoint_url,\n",
    "            region_name=self.region_name,\n",
    "            aws_access_key_id=self.access_key_id,\n",
    "            aws_secret_access_key=self.secret_access_key\n",
    "        )\n",
    "\n",
    "        # Configure multipart uploads to stay within S3 limits\n",
    "        self.transfer_config = TransferConfig(\n",
    "            multipart_threshold=8 * 1024 * 1024,\n",
    "            multipart_chunksize=128 * 1024 * 1024\n",
    "        )\n",
    "\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def list_buckets(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        List all available buckets.\n",
    "\n",
    "        Returns:\n",
    "            List of bucket names\n",
    "        \"\"\"\n",
    "        try:\n",
    "            buckets = [bucket.name for bucket in self.s3.buckets.all()]\n",
    "            return buckets\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error listing buckets: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_bucket(self, bucket_name: str, region: Optional[str] = None) -> bool:\n",
    "        \"\"\"\n",
    "        Create a new bucket.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket to create\n",
    "            region: Region to create the bucket in (optional)\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            create_bucket_config = {}\n",
    "            if region and region != 'us-east-1':\n",
    "                create_bucket_config['LocationConstraint'] = region\n",
    "\n",
    "            if create_bucket_config:\n",
    "                self.s3.create_bucket(\n",
    "                    Bucket=bucket_name,\n",
    "                    CreateBucketConfiguration=create_bucket_config\n",
    "                )\n",
    "            else:\n",
    "                self.s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "            self.logger.info(f\"Bucket {bucket_name} created successfully\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error creating bucket {bucket_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def delete_bucket(self, bucket_name: str, force: bool = False) -> bool:\n",
    "        \"\"\"\n",
    "        Delete a bucket. If force=True, will delete all objects first.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket to delete\n",
    "            force: If True, delete all objects in the bucket first\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bucket = self.s3.Bucket(bucket_name)\n",
    "\n",
    "            if force:\n",
    "                bucket.objects.all().delete()\n",
    "\n",
    "            bucket.delete()\n",
    "            self.logger.info(f\"Bucket {bucket_name} deleted successfully\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error deleting bucket {bucket_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def list_objects(self, bucket_name: str, prefix: str = '') -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        List objects in a bucket with optional prefix filter.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            prefix: Prefix to filter objects by\n",
    "\n",
    "        Returns:\n",
    "            List of objects with key, size, last_modified\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bucket = self.s3.Bucket(bucket_name)\n",
    "            objects = []\n",
    "\n",
    "            for obj in bucket.objects.filter(Prefix=prefix):\n",
    "                objects.append({\n",
    "                    'key': obj.key,\n",
    "                    'size': obj.size,\n",
    "                    'last_modified': obj.last_modified\n",
    "                })\n",
    "\n",
    "            return objects\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error listing objects in bucket {bucket_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def list_directories(self, bucket_name: str, prefix: str = '') -> List[str]:\n",
    "        \"\"\"\n",
    "        List directories (common prefixes) in a bucket under a specific prefix.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            prefix: Prefix to filter directories by (e.g., \"olkoa-projects/\")\n",
    "\n",
    "        Returns:\n",
    "            List of directory names (without the base prefix)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure prefix ends with '/' for proper directory listing\n",
    "            if prefix and not prefix.endswith('/'):\n",
    "                prefix += '/'\n",
    "\n",
    "            # Use list_objects_v2 with delimiter to get common prefixes (directories)\n",
    "            response = self.client.list_objects_v2(\n",
    "                Bucket=bucket_name,\n",
    "                Prefix=prefix,\n",
    "                Delimiter='/'\n",
    "            )\n",
    "\n",
    "            directories = []\n",
    "\n",
    "            # Extract directory names from CommonPrefixes\n",
    "            if 'CommonPrefixes' in response:\n",
    "                for prefix_info in response['CommonPrefixes']:\n",
    "                    dir_path = prefix_info['Prefix']\n",
    "                    # Remove the base prefix and trailing slash to get just the directory name\n",
    "                    dir_name = dir_path[len(prefix):].rstrip('/')\n",
    "                    if dir_name:  # Only add non-empty directory names\n",
    "                        directories.append(dir_name)\n",
    "\n",
    "            self.logger.info(f\"Found {len(directories)} directories in {bucket_name}/{prefix}\")\n",
    "            return directories\n",
    "\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error listing directories in bucket {bucket_name} with prefix {prefix}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def upload_file(self, file_path: str, bucket_name: str,\n",
    "                   object_key: Optional[str] = None,\n",
    "                   extra_args: Optional[Dict[str, Any]] = None,\n",
    "                   show_progress: bool = True) -> bool:\n",
    "        \"\"\"\n",
    "        Upload a file to S3.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the local file\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key to use in S3 (defaults to filename if not provided)\n",
    "            extra_args: Additional arguments for upload (ContentType, ACL, etc.)\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        if not object_key:\n",
    "            object_key = os.path.basename(file_path)\n",
    "\n",
    "        # Determine content type if not specified in extra_args\n",
    "        if extra_args is None:\n",
    "            extra_args = {}\n",
    "\n",
    "        if 'ContentType' not in extra_args:\n",
    "            content_type, _ = mimetypes.guess_type(file_path)\n",
    "            if content_type:\n",
    "                extra_args['ContentType'] = content_type\n",
    "\n",
    "        try:\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            max_parts = 1000\n",
    "            min_chunk_size = 8 * 1024 * 1024\n",
    "            suggested_chunk = max(min_chunk_size, math.ceil(file_size / max_parts))\n",
    "            max_chunk_size = 5 * 1024 * 1024 * 1024\n",
    "            chunk_size = min(suggested_chunk, max_chunk_size)\n",
    "\n",
    "            if chunk_size != self.transfer_config.multipart_chunksize:\n",
    "                transfer_config = TransferConfig(\n",
    "                    multipart_threshold=self.transfer_config.multipart_threshold,\n",
    "                    multipart_chunksize=chunk_size,\n",
    "                )\n",
    "            else:\n",
    "                transfer_config = self.transfer_config\n",
    "\n",
    "            callback = UploadProgress(file_path) if show_progress else None\n",
    "            self.s3.meta.client.upload_file(\n",
    "                file_path,\n",
    "                bucket_name,\n",
    "                object_key,\n",
    "                ExtraArgs=extra_args,\n",
    "                Config=transfer_config,\n",
    "                Callback=callback\n",
    "            )\n",
    "            self.logger.info(f\"File {file_path} uploaded to {bucket_name}/{object_key}\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error uploading file {file_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def upload_mailbox_raw(self, local_raw_data_dir: str, mailbox_name: str, project_bucket: str = \"olkoa-projects\", show_progress: bool = True) -> None:\n",
    "        \"\"\"Upload a mailbox raw directory to the configured project bucket.\"\"\"\n",
    "        buckets = self.list_buckets()\n",
    "        if project_bucket not in buckets:\n",
    "            self.create_bucket(project_bucket)\n",
    "            self.logger.info(\"Bucket '%s' created.\", project_bucket)\n",
    "        else:\n",
    "            self.logger.info(\"Bucket '%s' already exists.\", project_bucket)\n",
    "\n",
    "        s3_prefix = f\"{mailbox_name}/raw/\"\n",
    "        self.upload_directory(\n",
    "            local_dir=local_raw_data_dir,\n",
    "            bucket_name=project_bucket,\n",
    "            s3_prefix=s3_prefix,\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "\n",
    "    def upload_directory(self, local_dir, bucket_name, s3_prefix, show_progress: bool = True):\n",
    "        \"\"\"\n",
    "        Upload a directory and all its contents to S3, preserving the folder structure.\n",
    "\n",
    "        Args:\n",
    "            s3_handler: Instance of S3Handler class\n",
    "            local_dir: Path to local directory\n",
    "            bucket_name: Name of the S3 bucket\n",
    "            s3_prefix: Prefix in S3 where files should be uploaded\n",
    "        \"\"\"\n",
    "        for root, dirs, files in os.walk(local_dir):\n",
    "            for file in files:\n",
    "                local_file_path = os.path.join(root, file)\n",
    "\n",
    "                # Create S3 key by replacing local path with S3 prefix\n",
    "                relative_path = os.path.relpath(local_file_path, local_dir)\n",
    "                s3_key = os.path.join(s3_prefix, relative_path).replace(\"\\\\\", \"/\")\n",
    "\n",
    "                # Upload the file\n",
    "                self.upload_file(\n",
    "                    file_path=local_file_path,\n",
    "                    bucket_name=bucket_name,\n",
    "                    object_key=s3_key,\n",
    "                    show_progress=show_progress\n",
    "                )\n",
    "                print(f\"Uploaded {local_file_path} to {bucket_name}/{s3_key}\")\n",
    "\n",
    "    def upload_fileobj(self, file_obj: BinaryIO, bucket_name: str,\n",
    "                      object_key: str,\n",
    "                      extra_args: Optional[Dict[str, Any]] = None) -> bool:\n",
    "        \"\"\"\n",
    "        Upload a file-like object to S3.\n",
    "\n",
    "        Args:\n",
    "            file_obj: File-like object to upload\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key to use in S3\n",
    "            extra_args: Additional arguments for upload (ContentType, ACL, etc.)\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.s3.meta.client.upload_fileobj(\n",
    "                file_obj, bucket_name, object_key, ExtraArgs=extra_args or {}\n",
    "            )\n",
    "            self.logger.info(f\"File object uploaded to {bucket_name}/{object_key}\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error uploading file object: {e}\")\n",
    "            return False\n",
    "\n",
    "    def download_directory(self, bucket_name: str, s3_prefix: str, local_dir: str,\n",
    "                          progress_callback: Optional[callable] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Download all files from an S3 prefix (directory) to a local directory, preserving structure.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the S3 bucket\n",
    "            s3_prefix: S3 prefix (directory path) to download from\n",
    "            local_dir: Local directory to download files to\n",
    "            progress_callback: Optional callback function for progress updates\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with download statistics: {\n",
    "                'total_files': int,\n",
    "                'downloaded_files': int,\n",
    "                'failed_files': int,\n",
    "                'total_size': int,\n",
    "                'downloaded_paths': List[str],\n",
    "                'failed_paths': List[str]\n",
    "            }\n",
    "        \"\"\"\n",
    "        # Ensure local directory exists\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "        # Remove trailing slash from prefix if present\n",
    "        s3_prefix = s3_prefix.rstrip('/')\n",
    "\n",
    "        stats = {\n",
    "            'total_files': 0,\n",
    "            'downloaded_files': 0,\n",
    "            'failed_files': 0,\n",
    "            'total_size': 0,\n",
    "            'downloaded_paths': [],\n",
    "            'failed_paths': []\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # List all objects with the prefix\n",
    "            objects = self.list_objects(bucket_name, s3_prefix)\n",
    "            stats['total_files'] = len(objects)\n",
    "\n",
    "            if stats['total_files'] == 0:\n",
    "                self.logger.warning(f\"No files found in {bucket_name} with prefix '{s3_prefix}'\")\n",
    "                return stats\n",
    "\n",
    "            self.logger.info(f\"Starting download of {stats['total_files']} files from {bucket_name}/{s3_prefix} to {local_dir}\")\n",
    "\n",
    "            for i, obj in enumerate(objects):\n",
    "                try:\n",
    "                    s3_key = obj['key']\n",
    "\n",
    "                    # Create local file path by removing the s3_prefix and joining with local_dir\n",
    "                    if s3_key.startswith(s3_prefix + '/'):\n",
    "                        relative_path = s3_key[len(s3_prefix) + 1:]\n",
    "                    elif s3_key == s3_prefix:\n",
    "                        relative_path = os.path.basename(s3_key)\n",
    "                    else:\n",
    "                        # Handle case where s3_key contains the prefix but not as expected\n",
    "                        relative_path = s3_key.replace(s3_prefix, '').lstrip('/')\n",
    "\n",
    "                    local_file_path = os.path.join(local_dir, relative_path)\n",
    "\n",
    "                    # Create directory for the file if it doesn't exist\n",
    "                    os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "\n",
    "                    # Download the file\n",
    "                    success = self.download_file(bucket_name, s3_key, local_file_path)\n",
    "\n",
    "                    if success:\n",
    "                        stats['downloaded_files'] += 1\n",
    "                        stats['total_size'] += obj['size']\n",
    "                        stats['downloaded_paths'].append(local_file_path)\n",
    "                        self.logger.debug(f\"Downloaded {s3_key} to {local_file_path}\")\n",
    "                    else:\n",
    "                        stats['failed_files'] += 1\n",
    "                        stats['failed_paths'].append(s3_key)\n",
    "\n",
    "                    # Progress callback\n",
    "                    if progress_callback:\n",
    "                        progress_callback(i + 1, stats['total_files'], s3_key)\n",
    "\n",
    "                except Exception as e:\n",
    "                    stats['failed_files'] += 1\n",
    "                    stats['failed_paths'].append(obj['key'])\n",
    "                    self.logger.error(f\"Error downloading {obj['key']}: {e}\")\n",
    "\n",
    "            self.logger.info(f\"Download completed: {stats['downloaded_files']}/{stats['total_files']} files downloaded successfully\")\n",
    "            if stats['failed_files'] > 0:\n",
    "                self.logger.warning(f\"{stats['failed_files']} files failed to download\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error listing objects for download: {e}\")\n",
    "            raise\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def download_file(self, bucket_name: str, object_key: str,\n",
    "                     file_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Download a file from S3.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key of the object in S3\n",
    "            file_path: Path to save the file locally\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "            self.s3.meta.client.download_file(\n",
    "                bucket_name, object_key, file_path\n",
    "            )\n",
    "            self.logger.info(f\"File {bucket_name}/{object_key} downloaded to {file_path}\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error downloading file {bucket_name}/{object_key}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_object(self, bucket_name: str, object_key: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get an object and its metadata from S3.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key of the object in S3\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with object content and metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "            return {\n",
    "                'Body': response['Body'].read(),\n",
    "                'ContentType': response.get('ContentType'),\n",
    "                'ContentLength': response.get('ContentLength'),\n",
    "                'LastModified': response.get('LastModified'),\n",
    "                'Metadata': response.get('Metadata', {})\n",
    "            }\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error getting object {bucket_name}/{object_key}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def delete_object(self, bucket_name: str, object_key: str) -> bool:\n",
    "        \"\"\"\n",
    "        Delete an object from S3.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key of the object in S3\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.s3.Object(bucket_name, object_key).delete()\n",
    "            self.logger.info(f\"Object {bucket_name}/{object_key} deleted successfully\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error deleting object {bucket_name}/{object_key}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def delete_objects(self, bucket_name: str, object_keys: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Delete multiple objects from S3 in a single request.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            object_keys: List of object keys to delete\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with 'Deleted' and 'Errors' lists\n",
    "        \"\"\"\n",
    "        if not object_keys:\n",
    "            return {'Deleted': [], 'Errors': []}\n",
    "\n",
    "        try:\n",
    "            objects = [{'Key': key} for key in object_keys]\n",
    "            response = self.client.delete_objects(\n",
    "                Bucket=bucket_name,\n",
    "                Delete={'Objects': objects}\n",
    "            )\n",
    "\n",
    "            deleted = [obj.get('Key') for obj in response.get('Deleted', [])]\n",
    "            errors = [f\"{err.get('Key')}: {err.get('Message')}\" for err in response.get('Errors', [])]\n",
    "\n",
    "            if deleted:\n",
    "                self.logger.info(f\"Deleted {len(deleted)} objects from {bucket_name}\")\n",
    "            if errors:\n",
    "                self.logger.warning(f\"Failed to delete {len(errors)} objects from {bucket_name}\")\n",
    "\n",
    "            return {\n",
    "                'Deleted': deleted,\n",
    "                'Errors': errors\n",
    "            }\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error batch deleting objects from {bucket_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def copy_object(self, source_bucket: str, source_key: str,\n",
    "                   dest_bucket: str, dest_key: str) -> bool:\n",
    "        \"\"\"\n",
    "        Copy an object within S3.\n",
    "\n",
    "        Args:\n",
    "            source_bucket: Source bucket name\n",
    "            source_key: Source object key\n",
    "            dest_bucket: Destination bucket name\n",
    "            dest_key: Destination object key\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            copy_source = {\n",
    "                'Bucket': source_bucket,\n",
    "                'Key': source_key\n",
    "            }\n",
    "            self.s3.meta.client.copy(copy_source, dest_bucket, dest_key)\n",
    "            self.logger.info(f\"Object {source_bucket}/{source_key} copied to {dest_bucket}/{dest_key}\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error copying object: {e}\")\n",
    "            return False\n",
    "\n",
    "    def move_bucket_content(\n",
    "        self,\n",
    "        source_bucket: str,\n",
    "        dest_bucket: str,\n",
    "        source_prefix: str = '',\n",
    "        dest_prefix: str = '',\n",
    "        delete_source: bool = True,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Copy all objects from one bucket/prefix to another and delete the originals.\n",
    "\n",
    "        Args:\n",
    "            source_bucket: Bucket holding the objects to move.\n",
    "            dest_bucket: Bucket that will receive the copied objects.\n",
    "            source_prefix: Optional prefix filter for the source objects.\n",
    "            dest_prefix: Optional prefix prepended to each destination key.\n",
    "\n",
    "        Returns:\n",
    "            Summary dictionary with counts of copied/deleted files and any errors.\n",
    "        \"\"\"\n",
    "        objects = self.list_objects(source_bucket, prefix=source_prefix)\n",
    "\n",
    "        if not objects:\n",
    "            self.logger.info(\n",
    "                f\"No objects found to move from {source_bucket}/{source_prefix or ''}\"\n",
    "            )\n",
    "            return {\n",
    "                'copied': 0,\n",
    "                'deleted': 0,\n",
    "                'errors': [],\n",
    "            }\n",
    "\n",
    "        copied = 0\n",
    "        errors: List[str] = []\n",
    "        keys_to_delete: List[str] = []\n",
    "\n",
    "        # Normalise prefixes to avoid duplicate slashes\n",
    "        dest_prefix = dest_prefix.strip('/')\n",
    "        source_prefix = source_prefix.strip('/')\n",
    "\n",
    "        for obj in objects:\n",
    "            key = obj['key']\n",
    "            if not key:\n",
    "                continue\n",
    "\n",
    "            # Skip pseudo-directory markers\n",
    "            if key.endswith('/'):\n",
    "                continue\n",
    "\n",
    "            relative_key = key[len(source_prefix) + 1:] if source_prefix and key.startswith(source_prefix + '/') else key\n",
    "            dest_key = f\"{dest_prefix}/{relative_key}\" if dest_prefix else relative_key\n",
    "\n",
    "            if self.copy_object(source_bucket, key, dest_bucket, dest_key):\n",
    "                copied += 1\n",
    "                if delete_source:\n",
    "                    keys_to_delete.append(key)\n",
    "            else:\n",
    "                errors.append(f\"Failed to copy {source_bucket}/{key} to {dest_bucket}/{dest_key}\")\n",
    "\n",
    "        deleted = 0\n",
    "        if delete_source and keys_to_delete:\n",
    "            delete_result = self.delete_objects(source_bucket, keys_to_delete)\n",
    "            deleted = len(delete_result.get('Deleted', []))\n",
    "            if delete_result.get('Errors'):\n",
    "                errors.extend(delete_result['Errors'])\n",
    "\n",
    "        return {\n",
    "            'copied': copied,\n",
    "            'deleted': deleted,\n",
    "            'errors': errors,\n",
    "        }\n",
    "\n",
    "    def generate_presigned_url(self, bucket_name: str, object_key: str,\n",
    "                              expiration: int = 3600, http_method: str = 'GET') -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Generate a presigned URL for an S3 object.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key of the object in S3\n",
    "            expiration: Time in seconds until the URL expires\n",
    "            http_method: HTTP method to allow ('GET', 'PUT')\n",
    "\n",
    "        Returns:\n",
    "            Presigned URL or None if error\n",
    "        \"\"\"\n",
    "        try:\n",
    "            url = self.client.generate_presigned_url(\n",
    "                'get_object' if http_method == 'GET' else 'put_object',\n",
    "                Params={'Bucket': bucket_name, 'Key': object_key},\n",
    "                ExpiresIn=expiration\n",
    "            )\n",
    "            return url\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error generating presigned URL: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Example usage in main\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the handler\n",
    "    s3_handler = S3Handler()\n",
    "\n",
    "    try:\n",
    "        # List buckets\n",
    "        print(\"Available buckets:\")\n",
    "        buckets = s3_handler.list_buckets()\n",
    "        for bucket in buckets:\n",
    "            print(f\"- {bucket}\")\n",
    "\n",
    "        # if buckets:\n",
    "        #     # Pick the first bucket for demonstration\n",
    "        #     demo_bucket = buckets[0]\n",
    "        #     print(f\"\\nListing objects in bucket '{demo_bucket}':\")\n",
    "        #     objects = s3_handler.list_objects(demo_bucket)\n",
    "\n",
    "        #     for obj in objects[:10]:  # Show first 10 objects\n",
    "        #         print(f\"- {obj['key']} ({obj['size']} bytes, modified: {obj['last_modified']})\")\n",
    "\n",
    "        testerette_bucket = 'demo-testerette-bucket'\n",
    "\n",
    "        s3_handler.create_bucket(testerette_bucket)\n",
    "        buckets = s3_handler.list_buckets()\n",
    "        for bucket in buckets:\n",
    "            print(f\"- {bucket}\")\n",
    "\n",
    "        # Upload a file\n",
    "        # s3_handler.upload_file('mermaid.md', testerette_bucket, 'mermaid.md')\n",
    "        # print(\"\\nUploading file to bucket:\")\n",
    "        # objects = s3_handler.list_objects(testerette_bucket)\n",
    "\n",
    "        print(f\"\\nListing objects in bucket '{testerette_bucket}':\")\n",
    "        objects = s3_handler.list_objects(testerette_bucket)\n",
    "\n",
    "        # For a string\n",
    "        text_data = \"This is some text I want to upload\"\n",
    "        text_file_obj = io.BytesIO(text_data.encode('utf-8'))  # Convert to bytes and wrap in BytesIO\n",
    "\n",
    "        # Upload the string as a file\n",
    "        s3_handler.upload_fileobj(\n",
    "            file_obj=text_file_obj,\n",
    "            bucket_name=testerette_bucket,\n",
    "            object_key=\"my-text-file.txt\"\n",
    "        )\n",
    "\n",
    "        # For a JSON object\n",
    "        json_data = {\"name\": \"John\", \"age\": 30}\n",
    "        json_string = json.dumps(json_data)  # Convert to JSON string\n",
    "        json_file_obj = io.BytesIO(json_string.encode('utf-8'))  # Convert to bytes and wrap in BytesIO\n",
    "\n",
    "        # Upload the JSON as a file\n",
    "        s3_handler.upload_fileobj(\n",
    "            file_obj=json_file_obj,\n",
    "            bucket_name=testerette_bucket,\n",
    "            object_key=\"data.json\",\n",
    "            extra_args={\"ContentType\": \"application/json\"}  # Specify correct content type\n",
    "        )\n",
    "\n",
    "        # s3_handler.get_object(testerette_bucket, 'mermaid.md')\n",
    "        # # Get the object\n",
    "        # obj = s3_handler.get_object(testerette_bucket, 'mermaid.md')\n",
    "\n",
    "        # # The body is returned as bytes, so we need to decode it to a string\n",
    "        # # Assuming the content is UTF-8 encoded text\n",
    "        # content = obj['Body'].decode('utf-8')\n",
    "\n",
    "        # # Print the content\n",
    "        # print(\"File content:\", content)\n",
    "\n",
    "        # # Delete the object\n",
    "        # s3_handler.delete_object(testerette_bucket, 'mermaid.md')\n",
    "        print(\"\\nListing objects in bucket after deletion:\")\n",
    "        objects = s3_handler.list_objects(testerette_bucket)\n",
    "        for obj in objects:\n",
    "            print(f\"- {obj['key']} ({obj['size']} bytes, modified: {obj['last_modified']})\")\n",
    "\n",
    "        # Copy my-text-file.txt\n",
    "        s3_handler.copy_object(\n",
    "            source_bucket=testerette_bucket,\n",
    "            source_key='my-text-file.txt',\n",
    "            dest_bucket=testerette_bucket,\n",
    "            dest_key='copied-text-file.txt'\n",
    "        )\n",
    "\n",
    "        print(\"\\nListing objects in bucket after copying:\")\n",
    "        objects = s3_handler.list_objects(testerette_bucket)\n",
    "        for obj in objects:\n",
    "            print(f\"- {obj['key']} ({obj['size']} bytes, modified: {obj['last_modified']})\")\n",
    "\n",
    "\n",
    "        s3_handler.delete_bucket(testerette_bucket, force=True)\n",
    "\n",
    "        # List buckets again to confirm deletion\n",
    "        print(\"\\nAvailable buckets after deletion:\")\n",
    "\n",
    "        buckets = s3_handler.list_buckets()\n",
    "        for bucket in buckets:\n",
    "            print(f\"- {bucket}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "def upload_raw_data_to_s3(local_raw_data_dir, project_bucket, mailbox_name):\n",
    "\n",
    "\n",
    "    # Initialize S3 handler\n",
    "    s3_handler = S3Handler()\n",
    "\n",
    "    # List existing buckets\n",
    "    buckets = s3_handler.list_buckets()\n",
    "    print(\"Existing buckets:\", buckets)\n",
    "\n",
    "    # Define your project bucket name\n",
    "    project_bucket = project_bucket\n",
    "\n",
    "    # Create the bucket if it doesn't exist\n",
    "    if project_bucket not in buckets:\n",
    "        s3_handler.create_bucket(project_bucket)\n",
    "        print(f\"Bucket '{project_bucket}' created.\")\n",
    "    else:\n",
    "        print(f\"Bucket '{project_bucket}' already exists.\")\n",
    "\n",
    "    # Upload raw data directory to S3\n",
    "    local_raw_data_dir = \"data/Projects/Projet Demo/Boîte mail de Céline/raw/\"\n",
    "    s3_prefix = f\"{mailbox_name}/raw/\"\n",
    "\n",
    "    s3_handler.upload_directory(\n",
    "        local_dir=local_raw_data_dir,\n",
    "        bucket_name=project_bucket,\n",
    "        s3_prefix=s3_prefix\n",
    "    )\n",
    "    print(f\"Uploaded contents of '{local_raw_data_dir}' to 's3://{project_bucket}/{s3_prefix}'\")\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from dotenv import load_dotenv\n",
    "from botocore.exceptions import ClientError\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Union, BinaryIO\n",
    "import mimetypes\n",
    "\n",
    "import json\n",
    "import io\n",
    "import math\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "df = pd.read_csv(\n",
    "    'Metadata_Export_20251124_184630.csv',\n",
    "    sep=';',\n",
    "    quotechar='\"',\n",
    "    engine='python'\n",
    ")\n",
    "df['ext'] = df['Nom'].str.split('.').str[-1].str.lower()\n",
    "df[\"ext\"].value_counts()\n",
    "\n",
    "class UploadProgress:\n",
    "    \"\"\"Console progress reporter for S3 uploads.\"\"\"\n",
    "\n",
    "    def __init__(self, filename: str):\n",
    "        self.filename = filename\n",
    "        self.filesize = float(os.path.getsize(filename)) if os.path.exists(filename) else 0.0\n",
    "        self._seen_so_far = 0\n",
    "        self._lock = threading.Lock()\n",
    "        self._start_time = time.time()\n",
    "\n",
    "    def __call__(self, bytes_amount: int) -> None:\n",
    "        if self.filesize <= 0:\n",
    "            return\n",
    "        with self._lock:\n",
    "            self._seen_so_far += bytes_amount\n",
    "            elapsed = max(time.time() - self._start_time, 1e-6)\n",
    "            speed = self._seen_so_far / elapsed  # bytes/sec\n",
    "            remaining = max(self.filesize - self._seen_so_far, 0.0)\n",
    "            eta = remaining / speed if speed > 0 else float(\"inf\")\n",
    "            percentage = (self._seen_so_far / self.filesize) * 100.0\n",
    "\n",
    "            eta_str = f\"{eta:6.1f}s\" if math.isfinite(eta) else \"--.-s\"\n",
    "\n",
    "            line = (\n",
    "                f\"\\r[{os.path.basename(self.filename)}] \"\n",
    "                f\"{self._seen_so_far/1e6:8.1f} / {self.filesize/1e6:8.1f} MB \"\n",
    "                f\"({percentage:5.1f}%)  speed: {speed/1e6:5.1f} MB/s  ETA: {eta_str}\"\n",
    "            )\n",
    "            sys.stdout.write(line)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if self._seen_so_far >= self.filesize:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "\n",
    "class S3Handler:\n",
    "    \"\"\"\n",
    "    A class to handle common S3 operations using boto3.\n",
    "    This includes listing, creating, deleting buckets and objects,\n",
    "    uploading and downloading files, and generating presigned URLs.\n",
    "    It also supports using environment variables for configuration.\n",
    "    Environment variables:\n",
    "        - S3_ENDPOINT_URL: The endpoint URL for the S3 service\n",
    "        - S3_REGION_NAME: The region name for the S3 service\n",
    "        - SCW_ACCESS_KEY: Access key ID for authentication\n",
    "        - SCW_SECRET_KEY: Secret access key for authentication\n",
    "    Example usage:\n",
    "        s3_handler = S3Handler()\n",
    "        buckets = s3_handler.list_buckets()\n",
    "        print(buckets)\n",
    "        s3_handler.create_bucket('my-new-bucket')\n",
    "        s3_handler.upload_file('local_file.txt', 'my-new-bucket', 's3_file.txt')\n",
    "        url = s3_handler.generate_presigned_url('my-new-bucket', 's3_file.txt')\n",
    "        print(url)\n",
    "        s3_handler.download_file('my-new-bucket', 's3_file.txt', 'downloaded_file.txt')\n",
    "    This class requires the `boto3` and `python-dotenv` packages.\n",
    "    Install them using:\n",
    "        pip install boto3 python-dotenv\n",
    "    Ensure to set the environment variables or pass them as arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, endpoint_url=None, region_name=None,\n",
    "                 access_key_id=None, secret_access_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the S3 handler with optional credentials.\n",
    "        If not provided, will use environment variables.\n",
    "        \"\"\"\n",
    "        # Load environment variables if not done already\n",
    "        load_dotenv()\n",
    "\n",
    "        # Use provided credentials or fall back to environment variables\n",
    "        self.endpoint_url = endpoint_url or os.getenv(\"S3_ENDPOINT_URL\")\n",
    "        self.region_name = region_name or os.getenv(\"S3_REGION_NAME\")\n",
    "        self.access_key_id = access_key_id or os.getenv(\"SCW_ACCESS_KEY\")\n",
    "        self.secret_access_key = secret_access_key or os.getenv(\"SCW_SECRET_KEY\")\n",
    "\n",
    "        # Initialize S3 resource and client\n",
    "        self.s3 = boto3.resource(\n",
    "            service_name='s3',\n",
    "            endpoint_url=self.endpoint_url,\n",
    "            region_name=self.region_name,\n",
    "            aws_access_key_id=self.access_key_id,\n",
    "            aws_secret_access_key=self.secret_access_key\n",
    "        )\n",
    "\n",
    "        self.client = boto3.client(\n",
    "            service_name='s3',\n",
    "            endpoint_url=self.endpoint_url,\n",
    "            region_name=self.region_name,\n",
    "            aws_access_key_id=self.access_key_id,\n",
    "            aws_secret_access_key=self.secret_access_key\n",
    "        )\n",
    "\n",
    "        # Configure multipart uploads to stay within S3 limits\n",
    "        self.transfer_config = TransferConfig(\n",
    "            multipart_threshold=8 * 1024 * 1024,\n",
    "            multipart_chunksize=128 * 1024 * 1024\n",
    "        )\n",
    "\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def list_buckets(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        List all available buckets.\n",
    "\n",
    "        Returns:\n",
    "            List of bucket names\n",
    "        \"\"\"\n",
    "        try:\n",
    "            buckets = [bucket.name for bucket in self.s3.buckets.all()]\n",
    "            return buckets\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error listing buckets: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_bucket(self, bucket_name: str, region: Optional[str] = None) -> bool:\n",
    "        \"\"\"\n",
    "        Create a new bucket.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket to create\n",
    "            region: Region to create the bucket in (optional)\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            create_bucket_config = {}\n",
    "            if region and region != 'us-east-1':\n",
    "                create_bucket_config['LocationConstraint'] = region\n",
    "\n",
    "            if create_bucket_config:\n",
    "                self.s3.create_bucket(\n",
    "                    Bucket=bucket_name,\n",
    "                    CreateBucketConfiguration=create_bucket_config\n",
    "                )\n",
    "            else:\n",
    "                self.s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "            self.logger.info(f\"Bucket {bucket_name} created successfully\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error creating bucket {bucket_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def delete_bucket(self, bucket_name: str, force: bool = False) -> bool:\n",
    "        \"\"\"\n",
    "        Delete a bucket. If force=True, will delete all objects first.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket to delete\n",
    "            force: If True, delete all objects in the bucket first\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bucket = self.s3.Bucket(bucket_name)\n",
    "\n",
    "            if force:\n",
    "                bucket.objects.all().delete()\n",
    "\n",
    "            bucket.delete()\n",
    "            self.logger.info(f\"Bucket {bucket_name} deleted successfully\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error deleting bucket {bucket_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def list_objects(self, bucket_name: str, prefix: str = '') -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        List objects in a bucket with optional prefix filter.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            prefix: Prefix to filter objects by\n",
    "\n",
    "        Returns:\n",
    "            List of objects with key, size, last_modified\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bucket = self.s3.Bucket(bucket_name)\n",
    "            objects = []\n",
    "\n",
    "            for obj in bucket.objects.filter(Prefix=prefix):\n",
    "                objects.append({\n",
    "                    'key': obj.key,\n",
    "                    'size': obj.size,\n",
    "                    'last_modified': obj.last_modified\n",
    "                })\n",
    "\n",
    "            return objects\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error listing objects in bucket {bucket_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def list_directories(self, bucket_name: str, prefix: str = '') -> List[str]:\n",
    "        \"\"\"\n",
    "        List directories (common prefixes) in a bucket under a specific prefix.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            prefix: Prefix to filter directories by (e.g., \"olkoa-projects/\")\n",
    "\n",
    "        Returns:\n",
    "            List of directory names (without the base prefix)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure prefix ends with '/' for proper directory listing\n",
    "            if prefix and not prefix.endswith('/'):\n",
    "                prefix += '/'\n",
    "\n",
    "            # Use list_objects_v2 with delimiter to get common prefixes (directories)\n",
    "            response = self.client.list_objects_v2(\n",
    "                Bucket=bucket_name,\n",
    "                Prefix=prefix,\n",
    "                Delimiter='/'\n",
    "            )\n",
    "\n",
    "            directories = []\n",
    "\n",
    "            # Extract directory names from CommonPrefixes\n",
    "            if 'CommonPrefixes' in response:\n",
    "                for prefix_info in response['CommonPrefixes']:\n",
    "                    dir_path = prefix_info['Prefix']\n",
    "                    # Remove the base prefix and trailing slash to get just the directory name\n",
    "                    dir_name = dir_path[len(prefix):].rstrip('/')\n",
    "                    if dir_name:  # Only add non-empty directory names\n",
    "                        directories.append(dir_name)\n",
    "\n",
    "            self.logger.info(f\"Found {len(directories)} directories in {bucket_name}/{prefix}\")\n",
    "            return directories\n",
    "\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error listing directories in bucket {bucket_name} with prefix {prefix}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def upload_file(self, file_path: str, bucket_name: str,\n",
    "                   object_key: Optional[str] = None,\n",
    "                   extra_args: Optional[Dict[str, Any]] = None,\n",
    "                   show_progress: bool = True) -> bool:\n",
    "        \"\"\"\n",
    "        Upload a file to S3.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the local file\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key to use in S3 (defaults to filename if not provided)\n",
    "            extra_args: Additional arguments for upload (ContentType, ACL, etc.)\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        if not object_key:\n",
    "            object_key = os.path.basename(file_path)\n",
    "\n",
    "        # Determine content type if not specified in extra_args\n",
    "        if extra_args is None:\n",
    "            extra_args = {}\n",
    "\n",
    "        if 'ContentType' not in extra_args:\n",
    "            content_type, _ = mimetypes.guess_type(file_path)\n",
    "            if content_type:\n",
    "                extra_args['ContentType'] = content_type\n",
    "\n",
    "        try:\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            max_parts = 1000\n",
    "            min_chunk_size = 8 * 1024 * 1024\n",
    "            suggested_chunk = max(min_chunk_size, math.ceil(file_size / max_parts))\n",
    "            max_chunk_size = 5 * 1024 * 1024 * 1024\n",
    "            chunk_size = min(suggested_chunk, max_chunk_size)\n",
    "\n",
    "            if chunk_size != self.transfer_config.multipart_chunksize:\n",
    "                transfer_config = TransferConfig(\n",
    "                    multipart_threshold=self.transfer_config.multipart_threshold,\n",
    "                    multipart_chunksize=chunk_size,\n",
    "                )\n",
    "            else:\n",
    "                transfer_config = self.transfer_config\n",
    "\n",
    "            callback = UploadProgress(file_path) if show_progress else None\n",
    "            self.s3.meta.client.upload_file(\n",
    "                file_path,\n",
    "                bucket_name,\n",
    "                object_key,\n",
    "                ExtraArgs=extra_args,\n",
    "                Config=transfer_config,\n",
    "                Callback=callback\n",
    "            )\n",
    "            self.logger.info(f\"File {file_path} uploaded to {bucket_name}/{object_key}\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error uploading file {file_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def upload_mailbox_raw(self, local_raw_data_dir: str, mailbox_name: str, project_bucket: str = \"olkoa-projects\", show_progress: bool = True) -> None:\n",
    "        \"\"\"Upload a mailbox raw directory to the configured project bucket.\"\"\"\n",
    "        buckets = self.list_buckets()\n",
    "        if project_bucket not in buckets:\n",
    "            self.create_bucket(project_bucket)\n",
    "            self.logger.info(\"Bucket '%s' created.\", project_bucket)\n",
    "        else:\n",
    "            self.logger.info(\"Bucket '%s' already exists.\", project_bucket)\n",
    "\n",
    "        s3_prefix = f\"{mailbox_name}/raw/\"\n",
    "        self.upload_directory(\n",
    "            local_dir=local_raw_data_dir,\n",
    "            bucket_name=project_bucket,\n",
    "            s3_prefix=s3_prefix,\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "\n",
    "    def upload_directory(self, local_dir, bucket_name, s3_prefix, show_progress: bool = True):\n",
    "        \"\"\"\n",
    "        Upload a directory and all its contents to S3, preserving the folder structure.\n",
    "\n",
    "        Args:\n",
    "            s3_handler: Instance of S3Handler class\n",
    "            local_dir: Path to local directory\n",
    "            bucket_name: Name of the S3 bucket\n",
    "            s3_prefix: Prefix in S3 where files should be uploaded\n",
    "        \"\"\"\n",
    "        for root, dirs, files in os.walk(local_dir):\n",
    "            for file in files:\n",
    "                local_file_path = os.path.join(root, file)\n",
    "\n",
    "                # Create S3 key by replacing local path with S3 prefix\n",
    "                relative_path = os.path.relpath(local_file_path, local_dir)\n",
    "                s3_key = os.path.join(s3_prefix, relative_path).replace(\"\\\\\", \"/\")\n",
    "\n",
    "                # Upload the file\n",
    "                self.upload_file(\n",
    "                    file_path=local_file_path,\n",
    "                    bucket_name=bucket_name,\n",
    "                    object_key=s3_key,\n",
    "                    show_progress=show_progress\n",
    "                )\n",
    "                print(f\"Uploaded {local_file_path} to {bucket_name}/{s3_key}\")\n",
    "\n",
    "    def upload_fileobj(self, file_obj: BinaryIO, bucket_name: str,\n",
    "                      object_key: str,\n",
    "                      extra_args: Optional[Dict[str, Any]] = None) -> bool:\n",
    "        \"\"\"\n",
    "        Upload a file-like object to S3.\n",
    "\n",
    "        Args:\n",
    "            file_obj: File-like object to upload\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key to use in S3\n",
    "            extra_args: Additional arguments for upload (ContentType, ACL, etc.)\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.s3.meta.client.upload_fileobj(\n",
    "                file_obj, bucket_name, object_key, ExtraArgs=extra_args or {}\n",
    "            )\n",
    "            self.logger.info(f\"File object uploaded to {bucket_name}/{object_key}\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error uploading file object: {e}\")\n",
    "            return False\n",
    "\n",
    "    def download_directory(self, bucket_name: str, s3_prefix: str, local_dir: str,\n",
    "                          progress_callback: Optional[callable] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Download all files from an S3 prefix (directory) to a local directory, preserving structure.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the S3 bucket\n",
    "            s3_prefix: S3 prefix (directory path) to download from\n",
    "            local_dir: Local directory to download files to\n",
    "            progress_callback: Optional callback function for progress updates\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with download statistics: {\n",
    "                'total_files': int,\n",
    "                'downloaded_files': int,\n",
    "                'failed_files': int,\n",
    "                'total_size': int,\n",
    "                'downloaded_paths': List[str],\n",
    "                'failed_paths': List[str]\n",
    "            }\n",
    "        \"\"\"\n",
    "        # Ensure local directory exists\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "        # Remove trailing slash from prefix if present\n",
    "        s3_prefix = s3_prefix.rstrip('/')\n",
    "\n",
    "        stats = {\n",
    "            'total_files': 0,\n",
    "            'downloaded_files': 0,\n",
    "            'failed_files': 0,\n",
    "            'total_size': 0,\n",
    "            'downloaded_paths': [],\n",
    "            'failed_paths': []\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # List all objects with the prefix\n",
    "            objects = self.list_objects(bucket_name, s3_prefix)\n",
    "            stats['total_files'] = len(objects)\n",
    "\n",
    "            if stats['total_files'] == 0:\n",
    "                self.logger.warning(f\"No files found in {bucket_name} with prefix '{s3_prefix}'\")\n",
    "                return stats\n",
    "\n",
    "            self.logger.info(f\"Starting download of {stats['total_files']} files from {bucket_name}/{s3_prefix} to {local_dir}\")\n",
    "\n",
    "            for i, obj in enumerate(objects):\n",
    "                try:\n",
    "                    s3_key = obj['key']\n",
    "\n",
    "                    # Create local file path by removing the s3_prefix and joining with local_dir\n",
    "                    if s3_key.startswith(s3_prefix + '/'):\n",
    "                        relative_path = s3_key[len(s3_prefix) + 1:]\n",
    "                    elif s3_key == s3_prefix:\n",
    "                        relative_path = os.path.basename(s3_key)\n",
    "                    else:\n",
    "                        # Handle case where s3_key contains the prefix but not as expected\n",
    "                        relative_path = s3_key.replace(s3_prefix, '').lstrip('/')\n",
    "\n",
    "                    local_file_path = os.path.join(local_dir, relative_path)\n",
    "\n",
    "                    # Create directory for the file if it doesn't exist\n",
    "                    os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "\n",
    "                    # Download the file\n",
    "                    success = self.download_file(bucket_name, s3_key, local_file_path)\n",
    "\n",
    "                    if success:\n",
    "                        stats['downloaded_files'] += 1\n",
    "                        stats['total_size'] += obj['size']\n",
    "                        stats['downloaded_paths'].append(local_file_path)\n",
    "                        self.logger.debug(f\"Downloaded {s3_key} to {local_file_path}\")\n",
    "                    else:\n",
    "                        stats['failed_files'] += 1\n",
    "                        stats['failed_paths'].append(s3_key)\n",
    "\n",
    "                    # Progress callback\n",
    "                    if progress_callback:\n",
    "                        progress_callback(i + 1, stats['total_files'], s3_key)\n",
    "\n",
    "                except Exception as e:\n",
    "                    stats['failed_files'] += 1\n",
    "                    stats['failed_paths'].append(obj['key'])\n",
    "                    self.logger.error(f\"Error downloading {obj['key']}: {e}\")\n",
    "\n",
    "            self.logger.info(f\"Download completed: {stats['downloaded_files']}/{stats['total_files']} files downloaded successfully\")\n",
    "            if stats['failed_files'] > 0:\n",
    "                self.logger.warning(f\"{stats['failed_files']} files failed to download\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error listing objects for download: {e}\")\n",
    "            raise\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def download_file(self, bucket_name: str, object_key: str,\n",
    "                     file_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Download a file from S3.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key of the object in S3\n",
    "            file_path: Path to save the file locally\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "            self.s3.meta.client.download_file(\n",
    "                bucket_name, object_key, file_path\n",
    "            )\n",
    "            self.logger.info(f\"File {bucket_name}/{object_key} downloaded to {file_path}\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error downloading file {bucket_name}/{object_key}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_object(self, bucket_name: str, object_key: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get an object and its metadata from S3.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key of the object in S3\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with object content and metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "            return {\n",
    "                'Body': response['Body'].read(),\n",
    "                'ContentType': response.get('ContentType'),\n",
    "                'ContentLength': response.get('ContentLength'),\n",
    "                'LastModified': response.get('LastModified'),\n",
    "                'Metadata': response.get('Metadata', {})\n",
    "            }\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error getting object {bucket_name}/{object_key}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def delete_object(self, bucket_name: str, object_key: str) -> bool:\n",
    "        \"\"\"\n",
    "        Delete an object from S3.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key of the object in S3\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.s3.Object(bucket_name, object_key).delete()\n",
    "            self.logger.info(f\"Object {bucket_name}/{object_key} deleted successfully\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error deleting object {bucket_name}/{object_key}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def delete_objects(self, bucket_name: str, object_keys: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Delete multiple objects from S3 in a single request.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            object_keys: List of object keys to delete\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with 'Deleted' and 'Errors' lists\n",
    "        \"\"\"\n",
    "        if not object_keys:\n",
    "            return {'Deleted': [], 'Errors': []}\n",
    "\n",
    "        try:\n",
    "            objects = [{'Key': key} for key in object_keys]\n",
    "            response = self.client.delete_objects(\n",
    "                Bucket=bucket_name,\n",
    "                Delete={'Objects': objects}\n",
    "            )\n",
    "\n",
    "            deleted = [obj.get('Key') for obj in response.get('Deleted', [])]\n",
    "            errors = [f\"{err.get('Key')}: {err.get('Message')}\" for err in response.get('Errors', [])]\n",
    "\n",
    "            if deleted:\n",
    "                self.logger.info(f\"Deleted {len(deleted)} objects from {bucket_name}\")\n",
    "            if errors:\n",
    "                self.logger.warning(f\"Failed to delete {len(errors)} objects from {bucket_name}\")\n",
    "\n",
    "            return {\n",
    "                'Deleted': deleted,\n",
    "                'Errors': errors\n",
    "            }\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error batch deleting objects from {bucket_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def copy_object(self, source_bucket: str, source_key: str,\n",
    "                   dest_bucket: str, dest_key: str) -> bool:\n",
    "        \"\"\"\n",
    "        Copy an object within S3.\n",
    "\n",
    "        Args:\n",
    "            source_bucket: Source bucket name\n",
    "            source_key: Source object key\n",
    "            dest_bucket: Destination bucket name\n",
    "            dest_key: Destination object key\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            copy_source = {\n",
    "                'Bucket': source_bucket,\n",
    "                'Key': source_key\n",
    "            }\n",
    "            self.s3.meta.client.copy(copy_source, dest_bucket, dest_key)\n",
    "            self.logger.info(f\"Object {source_bucket}/{source_key} copied to {dest_bucket}/{dest_key}\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error copying object: {e}\")\n",
    "            return False\n",
    "\n",
    "    def move_bucket_content(\n",
    "        self,\n",
    "        source_bucket: str,\n",
    "        dest_bucket: str,\n",
    "        source_prefix: str = '',\n",
    "        dest_prefix: str = '',\n",
    "        delete_source: bool = True,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Copy all objects from one bucket/prefix to another and delete the originals.\n",
    "\n",
    "        Args:\n",
    "            source_bucket: Bucket holding the objects to move.\n",
    "            dest_bucket: Bucket that will receive the copied objects.\n",
    "            source_prefix: Optional prefix filter for the source objects.\n",
    "            dest_prefix: Optional prefix prepended to each destination key.\n",
    "\n",
    "        Returns:\n",
    "            Summary dictionary with counts of copied/deleted files and any errors.\n",
    "        \"\"\"\n",
    "        objects = self.list_objects(source_bucket, prefix=source_prefix)\n",
    "\n",
    "        if not objects:\n",
    "            self.logger.info(\n",
    "                f\"No objects found to move from {source_bucket}/{source_prefix or ''}\"\n",
    "            )\n",
    "            return {\n",
    "                'copied': 0,\n",
    "                'deleted': 0,\n",
    "                'errors': [],\n",
    "            }\n",
    "\n",
    "        copied = 0\n",
    "        errors: List[str] = []\n",
    "        keys_to_delete: List[str] = []\n",
    "\n",
    "        # Normalise prefixes to avoid duplicate slashes\n",
    "        dest_prefix = dest_prefix.strip('/')\n",
    "        source_prefix = source_prefix.strip('/')\n",
    "\n",
    "        for obj in objects:\n",
    "            key = obj['key']\n",
    "            if not key:\n",
    "                continue\n",
    "\n",
    "            # Skip pseudo-directory markers\n",
    "            if key.endswith('/'):\n",
    "                continue\n",
    "\n",
    "            relative_key = key[len(source_prefix) + 1:] if source_prefix and key.startswith(source_prefix + '/') else key\n",
    "            dest_key = f\"{dest_prefix}/{relative_key}\" if dest_prefix else relative_key\n",
    "\n",
    "            if self.copy_object(source_bucket, key, dest_bucket, dest_key):\n",
    "                copied += 1\n",
    "                if delete_source:\n",
    "                    keys_to_delete.append(key)\n",
    "            else:\n",
    "                errors.append(f\"Failed to copy {source_bucket}/{key} to {dest_bucket}/{dest_key}\")\n",
    "\n",
    "        deleted = 0\n",
    "        if delete_source and keys_to_delete:\n",
    "            delete_result = self.delete_objects(source_bucket, keys_to_delete)\n",
    "            deleted = len(delete_result.get('Deleted', []))\n",
    "            if delete_result.get('Errors'):\n",
    "                errors.extend(delete_result['Errors'])\n",
    "\n",
    "        return {\n",
    "            'copied': copied,\n",
    "            'deleted': deleted,\n",
    "            'errors': errors,\n",
    "        }\n",
    "\n",
    "    def generate_presigned_url(self, bucket_name: str, object_key: str,\n",
    "                              expiration: int = 3600, http_method: str = 'GET') -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Generate a presigned URL for an S3 object.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key of the object in S3\n",
    "            expiration: Time in seconds until the URL expires\n",
    "            http_method: HTTP method to allow ('GET', 'PUT')\n",
    "\n",
    "        Returns:\n",
    "            Presigned URL or None if error\n",
    "        \"\"\"\n",
    "        try:\n",
    "            url = self.client.generate_presigned_url(\n",
    "                'get_object' if http_method == 'GET' else 'put_object',\n",
    "                Params={'Bucket': bucket_name, 'Key': object_key},\n",
    "                ExpiresIn=expiration\n",
    "            )\n",
    "            return url\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error generating presigned URL: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Example usage in main\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "    # Initialize the handler\n",
    "    # s3_handler = S3Handler()\n",
    "\n",
    "    # try:\n",
    "    #     # List buckets\n",
    "    #     print(\"Available buckets:\")\n",
    "    #     buckets = s3_handler.list_buckets()\n",
    "    #     for bucket in buckets:\n",
    "    #         print(f\"- {bucket}\")\n",
    "\n",
    "    #     # if buckets:\n",
    "    #     #     # Pick the first bucket for demonstration\n",
    "    #     #     demo_bucket = buckets[0]\n",
    "    #     #     print(f\"\\nListing objects in bucket '{demo_bucket}':\")\n",
    "    #     #     objects = s3_handler.list_objects(demo_bucket)\n",
    "\n",
    "    #     #     for obj in objects[:10]:  # Show first 10 objects\n",
    "    #     #         print(f\"- {obj['key']} ({obj['size']} bytes, modified: {obj['last_modified']})\")\n",
    "\n",
    "    #     testerette_bucket = 'demo-testerette-bucket'\n",
    "\n",
    "    #     s3_handler.create_bucket(testerette_bucket)\n",
    "    #     buckets = s3_handler.list_buckets()\n",
    "    #     for bucket in buckets:\n",
    "    #         print(f\"- {bucket}\")\n",
    "\n",
    "    #     # Upload a file\n",
    "    #     # s3_handler.upload_file('mermaid.md', testerette_bucket, 'mermaid.md')\n",
    "    #     # print(\"\\nUploading file to bucket:\")\n",
    "    #     # objects = s3_handler.list_objects(testerette_bucket)\n",
    "\n",
    "    #     print(f\"\\nListing objects in bucket '{testerette_bucket}':\")\n",
    "    #     objects = s3_handler.list_objects(testerette_bucket)\n",
    "\n",
    "    #     # For a string\n",
    "    #     text_data = \"This is some text I want to upload\"\n",
    "    #     text_file_obj = io.BytesIO(text_data.encode('utf-8'))  # Convert to bytes and wrap in BytesIO\n",
    "\n",
    "    #     # Upload the string as a file\n",
    "    #     s3_handler.upload_fileobj(\n",
    "    #         file_obj=text_file_obj,\n",
    "    #         bucket_name=testerette_bucket,\n",
    "    #         object_key=\"my-text-file.txt\"\n",
    "    #     )\n",
    "\n",
    "    #     # For a JSON object\n",
    "    #     json_data = {\"name\": \"John\", \"age\": 30}\n",
    "    #     json_string = json.dumps(json_data)  # Convert to JSON string\n",
    "    #     json_file_obj = io.BytesIO(json_string.encode('utf-8'))  # Convert to bytes and wrap in BytesIO\n",
    "\n",
    "    #     # Upload the JSON as a file\n",
    "    #     s3_handler.upload_fileobj(\n",
    "    #         file_obj=json_file_obj,\n",
    "    #         bucket_name=testerette_bucket,\n",
    "    #         object_key=\"data.json\",\n",
    "    #         extra_args={\"ContentType\": \"application/json\"}  # Specify correct content type\n",
    "    #     )\n",
    "\n",
    "    #     # s3_handler.get_object(testerette_bucket, 'mermaid.md')\n",
    "    #     # # Get the object\n",
    "    #     # obj = s3_handler.get_object(testerette_bucket, 'mermaid.md')\n",
    "\n",
    "    #     # # The body is returned as bytes, so we need to decode it to a string\n",
    "    #     # # Assuming the content is UTF-8 encoded text\n",
    "    #     # content = obj['Body'].decode('utf-8')\n",
    "\n",
    "    #     # # Print the content\n",
    "    #     # print(\"File content:\", content)\n",
    "\n",
    "    #     # # Delete the object\n",
    "    #     # s3_handler.delete_object(testerette_bucket, 'mermaid.md')\n",
    "    #     print(\"\\nListing objects in bucket after deletion:\")\n",
    "    #     objects = s3_handler.list_objects(testerette_bucket)\n",
    "    #     for obj in objects:\n",
    "    #         print(f\"- {obj['key']} ({obj['size']} bytes, modified: {obj['last_modified']})\")\n",
    "\n",
    "    #     # Copy my-text-file.txt\n",
    "    #     s3_handler.copy_object(\n",
    "    #         source_bucket=testerette_bucket,\n",
    "    #         source_key='my-text-file.txt',\n",
    "    #         dest_bucket=testerette_bucket,\n",
    "    #         dest_key='copied-text-file.txt'\n",
    "    #     )\n",
    "\n",
    "    #     print(\"\\nListing objects in bucket after copying:\")\n",
    "    #     objects = s3_handler.list_objects(testerette_bucket)\n",
    "    #     for obj in objects:\n",
    "    #         print(f\"- {obj['key']} ({obj['size']} bytes, modified: {obj['last_modified']})\")\n",
    "\n",
    "\n",
    "    #     s3_handler.delete_bucket(testerette_bucket, force=True)\n",
    "\n",
    "    #     # List buckets again to confirm deletion\n",
    "    #     print(\"\\nAvailable buckets after deletion:\")\n",
    "\n",
    "    #     buckets = s3_handler.list_buckets()\n",
    "    #     for bucket in buckets:\n",
    "    #         print(f\"- {bucket}\")\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "def upload_raw_data_to_s3(local_raw_data_dir, project_bucket, mailbox_name):\n",
    "\n",
    "\n",
    "    # Initialize S3 handler\n",
    "    s3_handler = S3Handler()\n",
    "\n",
    "    # List existing buckets\n",
    "    buckets = s3_handler.list_buckets()\n",
    "    print(\"Existing buckets:\", buckets)\n",
    "\n",
    "    # Define your project bucket name\n",
    "    project_bucket = project_bucket\n",
    "\n",
    "    # Create the bucket if it doesn't exist\n",
    "    if project_bucket not in buckets:\n",
    "        s3_handler.create_bucket(project_bucket)\n",
    "        print(f\"Bucket '{project_bucket}' created.\")\n",
    "    else:\n",
    "        print(f\"Bucket '{project_bucket}' already exists.\")\n",
    "\n",
    "    # Upload raw data directory to S3\n",
    "    local_raw_data_dir = \"data/Projects/Projet Demo/Boîte mail de Céline/raw/\"\n",
    "    s3_prefix = f\"{mailbox_name}/raw/\"\n",
    "\n",
    "    s3_handler.upload_directory(\n",
    "        local_dir=local_raw_data_dir,\n",
    "        bucket_name=project_bucket,\n",
    "        s3_prefix=s3_prefix\n",
    "    )\n",
    "    print(f\"Uploaded contents of '{local_raw_data_dir}' to 's3://{project_bucket}/{s3_prefix}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a98054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error listing buckets: Unable to locate credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available buckets:\n",
      "Error: Unable to locate credentials\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class UploadProgress:\n",
    "    \"\"\"Console progress reporter for S3 uploads.\"\"\"\n",
    "\n",
    "    def __init__(self, filename: str):\n",
    "        self.filename = filename\n",
    "        self.filesize = float(os.path.getsize(filename)) if os.path.exists(filename) else 0.0\n",
    "        self._seen_so_far = 0\n",
    "        self._lock = threading.Lock()\n",
    "        self._start_time = time.time()\n",
    "\n",
    "    def __call__(self, bytes_amount: int) -> None:\n",
    "        if self.filesize <= 0:\n",
    "            return\n",
    "        with self._lock:\n",
    "            self._seen_so_far += bytes_amount\n",
    "            elapsed = max(time.time() - self._start_time, 1e-6)\n",
    "            speed = self._seen_so_far / elapsed  # bytes/sec\n",
    "            remaining = max(self.filesize - self._seen_so_far, 0.0)\n",
    "            eta = remaining / speed if speed > 0 else float(\"inf\")\n",
    "            percentage = (self._seen_so_far / self.filesize) * 100.0\n",
    "\n",
    "            eta_str = f\"{eta:6.1f}s\" if math.isfinite(eta) else \"--.-s\"\n",
    "\n",
    "            line = (\n",
    "                f\"\\r[{os.path.basename(self.filename)}] \"\n",
    "                f\"{self._seen_so_far/1e6:8.1f} / {self.filesize/1e6:8.1f} MB \"\n",
    "                f\"({percentage:5.1f}%)  speed: {speed/1e6:5.1f} MB/s  ETA: {eta_str}\"\n",
    "            )\n",
    "            sys.stdout.write(line)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if self._seen_so_far >= self.filesize:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "\n",
    "class S3Handler:\n",
    "    \"\"\"\n",
    "    A class to handle common S3 operations using boto3.\n",
    "    This includes listing, creating, deleting buckets and objects,\n",
    "    uploading and downloading files, and generating presigned URLs.\n",
    "    It also supports using environment variables for configuration.\n",
    "    Environment variables:\n",
    "        - S3_ENDPOINT_URL: The endpoint URL for the S3 service\n",
    "        - S3_REGION_NAME: The region name for the S3 service\n",
    "        - SCW_ACCESS_KEY: Access key ID for authentication\n",
    "        - SCW_SECRET_KEY: Secret access key for authentication\n",
    "    Example usage:\n",
    "        s3_handler = S3Handler()\n",
    "        buckets = s3_handler.list_buckets()\n",
    "        print(buckets)\n",
    "        s3_handler.create_bucket('my-new-bucket')\n",
    "        s3_handler.upload_file('local_file.txt', 'my-new-bucket', 's3_file.txt')\n",
    "        url = s3_handler.generate_presigned_url('my-new-bucket', 's3_file.txt')\n",
    "        print(url)\n",
    "        s3_handler.download_file('my-new-bucket', 's3_file.txt', 'downloaded_file.txt')\n",
    "    This class requires the `boto3` and `python-dotenv` packages.\n",
    "    Install them using:\n",
    "        pip install boto3 python-dotenv\n",
    "    Ensure to set the environment variables or pass them as arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, endpoint_url=None, region_name=None,\n",
    "                 access_key_id=None, secret_access_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the S3 handler with optional credentials.\n",
    "        If not provided, will use environment variables.\n",
    "        \"\"\"\n",
    "        # Load environment variables if not done already\n",
    "        load_dotenv()\n",
    "\n",
    "        # Use provided credentials or fall back to environment variables\n",
    "        self.endpoint_url = endpoint_url or os.getenv(\"S3_ENDPOINT_URL\")\n",
    "        self.region_name = region_name or os.getenv(\"S3_REGION_NAME\")\n",
    "        self.access_key_id = access_key_id or os.getenv(\"SCW_ACCESS_KEY\")\n",
    "        self.secret_access_key = secret_access_key or os.getenv(\"SCW_SECRET_KEY\")\n",
    "\n",
    "        # Initialize S3 resource and client\n",
    "        self.s3 = boto3.resource(\n",
    "            service_name='s3',\n",
    "            endpoint_url=self.endpoint_url,\n",
    "            region_name=self.region_name,\n",
    "            aws_access_key_id=self.access_key_id,\n",
    "            aws_secret_access_key=self.secret_access_key\n",
    "        )\n",
    "\n",
    "        self.client = boto3.client(\n",
    "            service_name='s3',\n",
    "            endpoint_url=self.endpoint_url,\n",
    "            region_name=self.region_name,\n",
    "            aws_access_key_id=self.access_key_id,\n",
    "            aws_secret_access_key=self.secret_access_key\n",
    "        )\n",
    "\n",
    "        # Configure multipart uploads to stay within S3 limits\n",
    "        self.transfer_config = TransferConfig(\n",
    "            multipart_threshold=8 * 1024 * 1024,\n",
    "            multipart_chunksize=128 * 1024 * 1024\n",
    "        )\n",
    "\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def list_buckets(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        List all available buckets.\n",
    "\n",
    "        Returns:\n",
    "            List of bucket names\n",
    "        \"\"\"\n",
    "        try:\n",
    "            buckets = [bucket.name for bucket in self.s3.buckets.all()]\n",
    "            return buckets\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error listing buckets: {e}\")\n",
    "            raise\n",
    "\n",
    "    def create_bucket(self, bucket_name: str, region: Optional[str] = None) -> bool:\n",
    "        \"\"\"\n",
    "        Create a new bucket.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket to create\n",
    "            region: Region to create the bucket in (optional)\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            create_bucket_config = {}\n",
    "            if region and region != 'us-east-1':\n",
    "                create_bucket_config['LocationConstraint'] = region\n",
    "\n",
    "            if create_bucket_config:\n",
    "                self.s3.create_bucket(\n",
    "                    Bucket=bucket_name,\n",
    "                    CreateBucketConfiguration=create_bucket_config\n",
    "                )\n",
    "            else:\n",
    "                self.s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "            self.logger.info(f\"Bucket {bucket_name} created successfully\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error creating bucket {bucket_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def delete_bucket(self, bucket_name: str, force: bool = False) -> bool:\n",
    "        \"\"\"\n",
    "        Delete a bucket. If force=True, will delete all objects first.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket to delete\n",
    "            force: If True, delete all objects in the bucket first\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bucket = self.s3.Bucket(bucket_name)\n",
    "\n",
    "            if force:\n",
    "                bucket.objects.all().delete()\n",
    "\n",
    "            bucket.delete()\n",
    "            self.logger.info(f\"Bucket {bucket_name} deleted successfully\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error deleting bucket {bucket_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def list_objects(self, bucket_name: str, prefix: str = '') -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        List objects in a bucket with optional prefix filter.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            prefix: Prefix to filter objects by\n",
    "\n",
    "        Returns:\n",
    "            List of objects with key, size, last_modified\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bucket = self.s3.Bucket(bucket_name)\n",
    "            objects = []\n",
    "\n",
    "            for obj in bucket.objects.filter(Prefix=prefix):\n",
    "                objects.append({\n",
    "                    'key': obj.key,\n",
    "                    'size': obj.size,\n",
    "                    'last_modified': obj.last_modified\n",
    "                })\n",
    "\n",
    "            return objects\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error listing objects in bucket {bucket_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def list_directories(self, bucket_name: str, prefix: str = '') -> List[str]:\n",
    "        \"\"\"\n",
    "        List directories (common prefixes) in a bucket under a specific prefix.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            prefix: Prefix to filter directories by (e.g., \"olkoa-projects/\")\n",
    "\n",
    "        Returns:\n",
    "            List of directory names (without the base prefix)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure prefix ends with '/' for proper directory listing\n",
    "            if prefix and not prefix.endswith('/'):\n",
    "                prefix += '/'\n",
    "\n",
    "            # Use list_objects_v2 with delimiter to get common prefixes (directories)\n",
    "            response = self.client.list_objects_v2(\n",
    "                Bucket=bucket_name,\n",
    "                Prefix=prefix,\n",
    "                Delimiter='/'\n",
    "            )\n",
    "\n",
    "            directories = []\n",
    "\n",
    "            # Extract directory names from CommonPrefixes\n",
    "            if 'CommonPrefixes' in response:\n",
    "                for prefix_info in response['CommonPrefixes']:\n",
    "                    dir_path = prefix_info['Prefix']\n",
    "                    # Remove the base prefix and trailing slash to get just the directory name\n",
    "                    dir_name = dir_path[len(prefix):].rstrip('/')\n",
    "                    if dir_name:  # Only add non-empty directory names\n",
    "                        directories.append(dir_name)\n",
    "\n",
    "            self.logger.info(f\"Found {len(directories)} directories in {bucket_name}/{prefix}\")\n",
    "            return directories\n",
    "\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error listing directories in bucket {bucket_name} with prefix {prefix}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def upload_file(self, file_path: str, bucket_name: str,\n",
    "                   object_key: Optional[str] = None,\n",
    "                   extra_args: Optional[Dict[str, Any]] = None,\n",
    "                   show_progress: bool = True) -> bool:\n",
    "        \"\"\"\n",
    "        Upload a file to S3.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the local file\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key to use in S3 (defaults to filename if not provided)\n",
    "            extra_args: Additional arguments for upload (ContentType, ACL, etc.)\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        if not object_key:\n",
    "            object_key = os.path.basename(file_path)\n",
    "\n",
    "        # Determine content type if not specified in extra_args\n",
    "        if extra_args is None:\n",
    "            extra_args = {}\n",
    "\n",
    "        if 'ContentType' not in extra_args:\n",
    "            content_type, _ = mimetypes.guess_type(file_path)\n",
    "            if content_type:\n",
    "                extra_args['ContentType'] = content_type\n",
    "\n",
    "        try:\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            max_parts = 1000\n",
    "            min_chunk_size = 8 * 1024 * 1024\n",
    "            suggested_chunk = max(min_chunk_size, math.ceil(file_size / max_parts))\n",
    "            max_chunk_size = 5 * 1024 * 1024 * 1024\n",
    "            chunk_size = min(suggested_chunk, max_chunk_size)\n",
    "\n",
    "            if chunk_size != self.transfer_config.multipart_chunksize:\n",
    "                transfer_config = TransferConfig(\n",
    "                    multipart_threshold=self.transfer_config.multipart_threshold,\n",
    "                    multipart_chunksize=chunk_size,\n",
    "                )\n",
    "            else:\n",
    "                transfer_config = self.transfer_config\n",
    "\n",
    "            callback = UploadProgress(file_path) if show_progress else None\n",
    "            self.s3.meta.client.upload_file(\n",
    "                file_path,\n",
    "                bucket_name,\n",
    "                object_key,\n",
    "                ExtraArgs=extra_args,\n",
    "                Config=transfer_config,\n",
    "                Callback=callback\n",
    "            )\n",
    "            self.logger.info(f\"File {file_path} uploaded to {bucket_name}/{object_key}\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error uploading file {file_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def upload_mailbox_raw(self, local_raw_data_dir: str, mailbox_name: str, project_bucket: str = \"olkoa-projects\", show_progress: bool = True) -> None:\n",
    "        \"\"\"Upload a mailbox raw directory to the configured project bucket.\"\"\"\n",
    "        buckets = self.list_buckets()\n",
    "        if project_bucket not in buckets:\n",
    "            self.create_bucket(project_bucket)\n",
    "            self.logger.info(\"Bucket '%s' created.\", project_bucket)\n",
    "        else:\n",
    "            self.logger.info(\"Bucket '%s' already exists.\", project_bucket)\n",
    "\n",
    "        s3_prefix = f\"{mailbox_name}/raw/\"\n",
    "        self.upload_directory(\n",
    "            local_dir=local_raw_data_dir,\n",
    "            bucket_name=project_bucket,\n",
    "            s3_prefix=s3_prefix,\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "\n",
    "    def upload_directory(self, local_dir, bucket_name, s3_prefix, show_progress: bool = True):\n",
    "        \"\"\"\n",
    "        Upload a directory and all its contents to S3, preserving the folder structure.\n",
    "\n",
    "        Args:\n",
    "            s3_handler: Instance of S3Handler class\n",
    "            local_dir: Path to local directory\n",
    "            bucket_name: Name of the S3 bucket\n",
    "            s3_prefix: Prefix in S3 where files should be uploaded\n",
    "        \"\"\"\n",
    "        for root, dirs, files in os.walk(local_dir):\n",
    "            for file in files:\n",
    "                local_file_path = os.path.join(root, file)\n",
    "\n",
    "                # Create S3 key by replacing local path with S3 prefix\n",
    "                relative_path = os.path.relpath(local_file_path, local_dir)\n",
    "                s3_key = os.path.join(s3_prefix, relative_path).replace(\"\\\\\", \"/\")\n",
    "\n",
    "                # Upload the file\n",
    "                self.upload_file(\n",
    "                    file_path=local_file_path,\n",
    "                    bucket_name=bucket_name,\n",
    "                    object_key=s3_key,\n",
    "                    show_progress=show_progress\n",
    "                )\n",
    "                print(f\"Uploaded {local_file_path} to {bucket_name}/{s3_key}\")\n",
    "\n",
    "    def upload_fileobj(self, file_obj: BinaryIO, bucket_name: str,\n",
    "                      object_key: str,\n",
    "                      extra_args: Optional[Dict[str, Any]] = None) -> bool:\n",
    "        \"\"\"\n",
    "        Upload a file-like object to S3.\n",
    "\n",
    "        Args:\n",
    "            file_obj: File-like object to upload\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key to use in S3\n",
    "            extra_args: Additional arguments for upload (ContentType, ACL, etc.)\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.s3.meta.client.upload_fileobj(\n",
    "                file_obj, bucket_name, object_key, ExtraArgs=extra_args or {}\n",
    "            )\n",
    "            self.logger.info(f\"File object uploaded to {bucket_name}/{object_key}\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error uploading file object: {e}\")\n",
    "            return False\n",
    "\n",
    "    def download_directory(self, bucket_name: str, s3_prefix: str, local_dir: str,\n",
    "                          progress_callback: Optional[callable] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Download all files from an S3 prefix (directory) to a local directory, preserving structure.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the S3 bucket\n",
    "            s3_prefix: S3 prefix (directory path) to download from\n",
    "            local_dir: Local directory to download files to\n",
    "            progress_callback: Optional callback function for progress updates\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with download statistics: {\n",
    "                'total_files': int,\n",
    "                'downloaded_files': int,\n",
    "                'failed_files': int,\n",
    "                'total_size': int,\n",
    "                'downloaded_paths': List[str],\n",
    "                'failed_paths': List[str]\n",
    "            }\n",
    "        \"\"\"\n",
    "        # Ensure local directory exists\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "        # Remove trailing slash from prefix if present\n",
    "        s3_prefix = s3_prefix.rstrip('/')\n",
    "\n",
    "        stats = {\n",
    "            'total_files': 0,\n",
    "            'downloaded_files': 0,\n",
    "            'failed_files': 0,\n",
    "            'total_size': 0,\n",
    "            'downloaded_paths': [],\n",
    "            'failed_paths': []\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # List all objects with the prefix\n",
    "            objects = self.list_objects(bucket_name, s3_prefix)\n",
    "            stats['total_files'] = len(objects)\n",
    "\n",
    "            if stats['total_files'] == 0:\n",
    "                self.logger.warning(f\"No files found in {bucket_name} with prefix '{s3_prefix}'\")\n",
    "                return stats\n",
    "\n",
    "            self.logger.info(f\"Starting download of {stats['total_files']} files from {bucket_name}/{s3_prefix} to {local_dir}\")\n",
    "\n",
    "            for i, obj in enumerate(objects):\n",
    "                try:\n",
    "                    s3_key = obj['key']\n",
    "\n",
    "                    # Create local file path by removing the s3_prefix and joining with local_dir\n",
    "                    if s3_key.startswith(s3_prefix + '/'):\n",
    "                        relative_path = s3_key[len(s3_prefix) + 1:]\n",
    "                    elif s3_key == s3_prefix:\n",
    "                        relative_path = os.path.basename(s3_key)\n",
    "                    else:\n",
    "                        # Handle case where s3_key contains the prefix but not as expected\n",
    "                        relative_path = s3_key.replace(s3_prefix, '').lstrip('/')\n",
    "\n",
    "                    local_file_path = os.path.join(local_dir, relative_path)\n",
    "\n",
    "                    # Create directory for the file if it doesn't exist\n",
    "                    os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "\n",
    "                    # Download the file\n",
    "                    success = self.download_file(bucket_name, s3_key, local_file_path)\n",
    "\n",
    "                    if success:\n",
    "                        stats['downloaded_files'] += 1\n",
    "                        stats['total_size'] += obj['size']\n",
    "                        stats['downloaded_paths'].append(local_file_path)\n",
    "                        self.logger.debug(f\"Downloaded {s3_key} to {local_file_path}\")\n",
    "                    else:\n",
    "                        stats['failed_files'] += 1\n",
    "                        stats['failed_paths'].append(s3_key)\n",
    "\n",
    "                    # Progress callback\n",
    "                    if progress_callback:\n",
    "                        progress_callback(i + 1, stats['total_files'], s3_key)\n",
    "\n",
    "                except Exception as e:\n",
    "                    stats['failed_files'] += 1\n",
    "                    stats['failed_paths'].append(obj['key'])\n",
    "                    self.logger.error(f\"Error downloading {obj['key']}: {e}\")\n",
    "\n",
    "            self.logger.info(f\"Download completed: {stats['downloaded_files']}/{stats['total_files']} files downloaded successfully\")\n",
    "            if stats['failed_files'] > 0:\n",
    "                self.logger.warning(f\"{stats['failed_files']} files failed to download\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error listing objects for download: {e}\")\n",
    "            raise\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def download_file(self, bucket_name: str, object_key: str,\n",
    "                     file_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Download a file from S3.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key of the object in S3\n",
    "            file_path: Path to save the file locally\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "            self.s3.meta.client.download_file(\n",
    "                bucket_name, object_key, file_path\n",
    "            )\n",
    "            self.logger.info(f\"File {bucket_name}/{object_key} downloaded to {file_path}\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error downloading file {bucket_name}/{object_key}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_object(self, bucket_name: str, object_key: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get an object and its metadata from S3.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key of the object in S3\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with object content and metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "            return {\n",
    "                'Body': response['Body'].read(),\n",
    "                'ContentType': response.get('ContentType'),\n",
    "                'ContentLength': response.get('ContentLength'),\n",
    "                'LastModified': response.get('LastModified'),\n",
    "                'Metadata': response.get('Metadata', {})\n",
    "            }\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error getting object {bucket_name}/{object_key}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def delete_object(self, bucket_name: str, object_key: str) -> bool:\n",
    "        \"\"\"\n",
    "        Delete an object from S3.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key of the object in S3\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.s3.Object(bucket_name, object_key).delete()\n",
    "            self.logger.info(f\"Object {bucket_name}/{object_key} deleted successfully\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error deleting object {bucket_name}/{object_key}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def delete_objects(self, bucket_name: str, object_keys: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Delete multiple objects from S3 in a single request.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            object_keys: List of object keys to delete\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with 'Deleted' and 'Errors' lists\n",
    "        \"\"\"\n",
    "        if not object_keys:\n",
    "            return {'Deleted': [], 'Errors': []}\n",
    "\n",
    "        try:\n",
    "            objects = [{'Key': key} for key in object_keys]\n",
    "            response = self.client.delete_objects(\n",
    "                Bucket=bucket_name,\n",
    "                Delete={'Objects': objects}\n",
    "            )\n",
    "\n",
    "            deleted = [obj.get('Key') for obj in response.get('Deleted', [])]\n",
    "            errors = [f\"{err.get('Key')}: {err.get('Message')}\" for err in response.get('Errors', [])]\n",
    "\n",
    "            if deleted:\n",
    "                self.logger.info(f\"Deleted {len(deleted)} objects from {bucket_name}\")\n",
    "            if errors:\n",
    "                self.logger.warning(f\"Failed to delete {len(errors)} objects from {bucket_name}\")\n",
    "\n",
    "            return {\n",
    "                'Deleted': deleted,\n",
    "                'Errors': errors\n",
    "            }\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error batch deleting objects from {bucket_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def copy_object(self, source_bucket: str, source_key: str,\n",
    "                   dest_bucket: str, dest_key: str) -> bool:\n",
    "        \"\"\"\n",
    "        Copy an object within S3.\n",
    "\n",
    "        Args:\n",
    "            source_bucket: Source bucket name\n",
    "            source_key: Source object key\n",
    "            dest_bucket: Destination bucket name\n",
    "            dest_key: Destination object key\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            copy_source = {\n",
    "                'Bucket': source_bucket,\n",
    "                'Key': source_key\n",
    "            }\n",
    "            self.s3.meta.client.copy(copy_source, dest_bucket, dest_key)\n",
    "            self.logger.info(f\"Object {source_bucket}/{source_key} copied to {dest_bucket}/{dest_key}\")\n",
    "            return True\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error copying object: {e}\")\n",
    "            return False\n",
    "\n",
    "    def move_bucket_content(\n",
    "        self,\n",
    "        source_bucket: str,\n",
    "        dest_bucket: str,\n",
    "        source_prefix: str = '',\n",
    "        dest_prefix: str = '',\n",
    "        delete_source: bool = True,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Copy all objects from one bucket/prefix to another and delete the originals.\n",
    "\n",
    "        Args:\n",
    "            source_bucket: Bucket holding the objects to move.\n",
    "            dest_bucket: Bucket that will receive the copied objects.\n",
    "            source_prefix: Optional prefix filter for the source objects.\n",
    "            dest_prefix: Optional prefix prepended to each destination key.\n",
    "\n",
    "        Returns:\n",
    "            Summary dictionary with counts of copied/deleted files and any errors.\n",
    "        \"\"\"\n",
    "        objects = self.list_objects(source_bucket, prefix=source_prefix)\n",
    "\n",
    "        if not objects:\n",
    "            self.logger.info(\n",
    "                f\"No objects found to move from {source_bucket}/{source_prefix or ''}\"\n",
    "            )\n",
    "            return {\n",
    "                'copied': 0,\n",
    "                'deleted': 0,\n",
    "                'errors': [],\n",
    "            }\n",
    "\n",
    "        copied = 0\n",
    "        errors: List[str] = []\n",
    "        keys_to_delete: List[str] = []\n",
    "\n",
    "        # Normalise prefixes to avoid duplicate slashes\n",
    "        dest_prefix = dest_prefix.strip('/')\n",
    "        source_prefix = source_prefix.strip('/')\n",
    "\n",
    "        for obj in objects:\n",
    "            key = obj['key']\n",
    "            if not key:\n",
    "                continue\n",
    "\n",
    "            # Skip pseudo-directory markers\n",
    "            if key.endswith('/'):\n",
    "                continue\n",
    "\n",
    "            relative_key = key[len(source_prefix) + 1:] if source_prefix and key.startswith(source_prefix + '/') else key\n",
    "            dest_key = f\"{dest_prefix}/{relative_key}\" if dest_prefix else relative_key\n",
    "\n",
    "            if self.copy_object(source_bucket, key, dest_bucket, dest_key):\n",
    "                copied += 1\n",
    "                if delete_source:\n",
    "                    keys_to_delete.append(key)\n",
    "            else:\n",
    "                errors.append(f\"Failed to copy {source_bucket}/{key} to {dest_bucket}/{dest_key}\")\n",
    "\n",
    "        deleted = 0\n",
    "        if delete_source and keys_to_delete:\n",
    "            delete_result = self.delete_objects(source_bucket, keys_to_delete)\n",
    "            deleted = len(delete_result.get('Deleted', []))\n",
    "            if delete_result.get('Errors'):\n",
    "                errors.extend(delete_result['Errors'])\n",
    "\n",
    "        return {\n",
    "            'copied': copied,\n",
    "            'deleted': deleted,\n",
    "            'errors': errors,\n",
    "        }\n",
    "\n",
    "    def generate_presigned_url(self, bucket_name: str, object_key: str,\n",
    "                              expiration: int = 3600, http_method: str = 'GET') -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Generate a presigned URL for an S3 object.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: Name of the bucket\n",
    "            object_key: Key of the object in S3\n",
    "            expiration: Time in seconds until the URL expires\n",
    "            http_method: HTTP method to allow ('GET', 'PUT')\n",
    "\n",
    "        Returns:\n",
    "            Presigned URL or None if error\n",
    "        \"\"\"\n",
    "        try:\n",
    "            url = self.client.generate_presigned_url(\n",
    "                'get_object' if http_method == 'GET' else 'put_object',\n",
    "                Params={'Bucket': bucket_name, 'Key': object_key},\n",
    "                ExpiresIn=expiration\n",
    "            )\n",
    "            return url\n",
    "        except ClientError as e:\n",
    "            self.logger.error(f\"Error generating presigned URL: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Example usage in main\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the handler\n",
    "    s3_handler = S3Handler()\n",
    "\n",
    "    try:\n",
    "        # List buckets\n",
    "        print(\"Available buckets:\")\n",
    "        buckets = s3_handler.list_buckets()\n",
    "        for bucket in buckets:\n",
    "            print(f\"- {bucket}\")\n",
    "\n",
    "        # if buckets:\n",
    "        #     # Pick the first bucket for demonstration\n",
    "        #     demo_bucket = buckets[0]\n",
    "        #     print(f\"\\nListing objects in bucket '{demo_bucket}':\")\n",
    "        #     objects = s3_handler.list_objects(demo_bucket)\n",
    "\n",
    "        #     for obj in objects[:10]:  # Show first 10 objects\n",
    "        #         print(f\"- {obj['key']} ({obj['size']} bytes, modified: {obj['last_modified']})\")\n",
    "\n",
    "        testerette_bucket = 'demo-testerette-bucket'\n",
    "\n",
    "        s3_handler.create_bucket(testerette_bucket)\n",
    "        buckets = s3_handler.list_buckets()\n",
    "        for bucket in buckets:\n",
    "            print(f\"- {bucket}\")\n",
    "\n",
    "        # Upload a file\n",
    "        # s3_handler.upload_file('mermaid.md', testerette_bucket, 'mermaid.md')\n",
    "        # print(\"\\nUploading file to bucket:\")\n",
    "        # objects = s3_handler.list_objects(testerette_bucket)\n",
    "\n",
    "        print(f\"\\nListing objects in bucket '{testerette_bucket}':\")\n",
    "        objects = s3_handler.list_objects(testerette_bucket)\n",
    "\n",
    "        # For a string\n",
    "        text_data = \"This is some text I want to upload\"\n",
    "        text_file_obj = io.BytesIO(text_data.encode('utf-8'))  # Convert to bytes and wrap in BytesIO\n",
    "\n",
    "        # Upload the string as a file\n",
    "        s3_handler.upload_fileobj(\n",
    "            file_obj=text_file_obj,\n",
    "            bucket_name=testerette_bucket,\n",
    "            object_key=\"my-text-file.txt\"\n",
    "        )\n",
    "\n",
    "        # For a JSON object\n",
    "        json_data = {\"name\": \"John\", \"age\": 30}\n",
    "        json_string = json.dumps(json_data)  # Convert to JSON string\n",
    "        json_file_obj = io.BytesIO(json_string.encode('utf-8'))  # Convert to bytes and wrap in BytesIO\n",
    "\n",
    "        # Upload the JSON as a file\n",
    "        s3_handler.upload_fileobj(\n",
    "            file_obj=json_file_obj,\n",
    "            bucket_name=testerette_bucket,\n",
    "            object_key=\"data.json\",\n",
    "            extra_args={\"ContentType\": \"application/json\"}  # Specify correct content type\n",
    "        )\n",
    "\n",
    "        # s3_handler.get_object(testerette_bucket, 'mermaid.md')\n",
    "        # # Get the object\n",
    "        # obj = s3_handler.get_object(testerette_bucket, 'mermaid.md')\n",
    "\n",
    "        # # The body is returned as bytes, so we need to decode it to a string\n",
    "        # # Assuming the content is UTF-8 encoded text\n",
    "        # content = obj['Body'].decode('utf-8')\n",
    "\n",
    "        # # Print the content\n",
    "        # print(\"File content:\", content)\n",
    "\n",
    "        # # Delete the object\n",
    "        # s3_handler.delete_object(testerette_bucket, 'mermaid.md')\n",
    "        print(\"\\nListing objects in bucket after deletion:\")\n",
    "        objects = s3_handler.list_objects(testerette_bucket)\n",
    "        for obj in objects:\n",
    "            print(f\"- {obj['key']} ({obj['size']} bytes, modified: {obj['last_modified']})\")\n",
    "\n",
    "        # Copy my-text-file.txt\n",
    "        s3_handler.copy_object(\n",
    "            source_bucket=testerette_bucket,\n",
    "            source_key='my-text-file.txt',\n",
    "            dest_bucket=testerette_bucket,\n",
    "            dest_key='copied-text-file.txt'\n",
    "        )\n",
    "\n",
    "        print(\"\\nListing objects in bucket after copying:\")\n",
    "        objects = s3_handler.list_objects(testerette_bucket)\n",
    "        for obj in objects:\n",
    "            print(f\"- {obj['key']} ({obj['size']} bytes, modified: {obj['last_modified']})\")\n",
    "\n",
    "\n",
    "        s3_handler.delete_bucket(testerette_bucket, force=True)\n",
    "\n",
    "        # List buckets again to confirm deletion\n",
    "        print(\"\\nAvailable buckets after deletion:\")\n",
    "\n",
    "        buckets = s3_handler.list_buckets()\n",
    "        for bucket in buckets:\n",
    "            print(f\"- {bucket}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "def upload_raw_data_to_s3(local_raw_data_dir, project_bucket, mailbox_name):\n",
    "\n",
    "\n",
    "    # Initialize S3 handler\n",
    "    s3_handler = S3Handler()\n",
    "\n",
    "    # List existing buckets\n",
    "    buckets = s3_handler.list_buckets()\n",
    "    print(\"Existing buckets:\", buckets)\n",
    "\n",
    "    # Define your project bucket name\n",
    "    project_bucket = project_bucket\n",
    "\n",
    "    # Create the bucket if it doesn't exist\n",
    "    if project_bucket not in buckets:\n",
    "        s3_handler.create_bucket(project_bucket)\n",
    "        print(f\"Bucket '{project_bucket}' created.\")\n",
    "    else:\n",
    "        print(f\"Bucket '{project_bucket}' already exists.\")\n",
    "\n",
    "    # Upload raw data directory to S3\n",
    "    local_raw_data_dir = \"data/Projects/Projet Demo/Boîte mail de Céline/raw/\"\n",
    "    s3_prefix = f\"{mailbox_name}/raw/\"\n",
    "\n",
    "    s3_handler.upload_directory(\n",
    "        local_dir=local_raw_data_dir,\n",
    "        bucket_name=project_bucket,\n",
    "        s3_prefix=s3_prefix\n",
    "    )\n",
    "    print(f\"Uploaded contents of '{local_raw_data_dir}' to 's3://{project_bucket}/{s3_prefix}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eec67b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
